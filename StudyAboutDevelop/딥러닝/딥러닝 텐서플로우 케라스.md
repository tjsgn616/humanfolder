[애플코딩](https://www.youtube.com/watch?v=ivfp2wpPLzs&list=PLfLgtT94nNq1DrREU_qG2w4yd2ZzJb-FG&ab_channel=%EC%BD%94%EB%94%A9%EC%95%A0%ED%94%8C)


### 추천 서비스
- [ms에서만든 스케치를 보고 html, css짜주는 서비스](sketch2code.azurewebsites.net)
- [adobe에서만든 ProjectAboutFace](합성된 얼굴사진인지 알려줌. 합성전의 사진까지 추측해줌)
- [구글의 알파고]
- [DALLE-2] (https://openai.com/dall-e-2/)어떠한 무엇이라고 텍스트를 입력하면 이미지를 그려줌
    가격 : 처음3개월동안 쓸수있는 18$ 무료크레딧줌. 대부분의 모델에서 2050개의 토큰. 1토

## 딥러닝
- 정의 : 빡대가리도 할 수 있는 머신러닝 기법
    전통적인 머신러닝은 사람이 준 가이드라인이 필요한데 딥러닝은 가이드라인도 컴퓨터가 찾는다.
    대신에 엄청난 데이터를 줘야함
- 정확한 정의 : 뉴럴 네트워크(뇌신경망을 본딴것)를 이용해 머신러닝을 진행하는 것
- 장점
    사전지식이 필요없다. 도메인 놀리지가 필요없다.
    추상적인 정보를 인식하는 능력이 뛰어남
- 딥러닝이 어울리는 분야
    이미지 분류, 물체 인식 (자율주행)
    순서가 있는 데이터 분석, 예측 (번역)
- 점과 선이 있는 perceptron 을 여러개 연결. layer를 거쳐서 결과
레이어로 모여있는것을 Neural Network(신경망) 이라고 한다.

#### 머신러닝 vs 딥러닝
- 머신러닝 : 부정확한 예측을 하면 엔지니어가 개입하여 조정. 많은 양의 데이터가 없을 때 활용.
- 딥러닝 : 신경망을 통해 예측 정확성 여부를 스스로 판단. 많은 양의 데이터가 주어진 경우에 활용.

### 머신러닝의 종류
- Supervised Learning : 데이터에 정답이 있고 정답예측 모델 만들때
- Unsupervised Learning : 데이터에 정답이 없을때 컴퓨터야 알아서 분류좀 해줘. 군집화 (추천서비스들)
- Reinforcement Learning(강화학습) : 컴퓨터에게 게임처럼 가르침. 잘하면 보상, 못하면 페널티. 

### 뉴럴 네트워크
인간은 뉴런과 시냅스를 통해 생각. 기억.
이 동작방식을 본따오면 기계도 사람처럼 생각할 수 있지 않을까? 하는데서 출발
- 히든레이어 : 기계가 잠깐 생각하고 저장하는 곳. hidden layer라서 H로 표현한다.
- 동그라미를 노드라고 한다.
- 가중치를 W로 표현한다. weight의 약자.
- 연결된 이전의 노드들을 다 더하고 가중치를 곱한다음 다 더하면 현재 노드에 들어갈 숫자다.
- 예 : 4월부터 9월까지 모의고사 점수로 수능점수 예측.
    4,5,6월은 초반점수 가중치 0.3
    7,8,9월은 후반점수 가중치 0.7
    최종 수능점수
- 예2 : 사람얼굴 판단
    동그란것들을 모아서 눈이 있는지 판단
- 오차 : 평균제곱오차   1/n  x 시그마  x (예측값-실제값)제곱
- 확률을 예측할때 쓰는 함수 : - 1/n 시그마 [실제값log(예측값)+(1-실제값)log(1-예측값)]
   복잡하지만 텐서플로우 쓰면 코드한줄이면 된다.
- 현재까지의 치명적인 단점 : 히든레이어가 있든 없든 결과값이 같다.
- 해결방법 : #### 활성함수 (activation function)
        수식을 넣어 짜부라뜨린다.
        대표적인게 sigmoid함수.  1/ 1+ e의 -x승
        Hyperbolic tangent, Rectified Linear Units

- 활성함수없이 예측 : 선형적이고 단순한 예측
  활성함수를 포함한 예측 : 비선형적이고 복잡한 예측 가능

#### 신나는 경사하강법
1. w값을 랜덤으로 찍음
2. w값을 바탕으로 총손실 E를 계산함
3. 경사하강으로 새로운 W값 업데이트
계속 2,3 반복. 총손실E가 더이상 안줄어들 때까지

경사하강법 = w값을 최적으로 변경하기 위해 경사를 타고 하강하는 기술.
          = 현재 W1값 - 접선의기울기
한사이클 제일 아래지점이 되면 기울기가 0이 되어서 반복이 끝나버린다.
그것을 막기위해 기울기만 빼지말고 learning rate * 기울기를 빼게된다.
learning rate는 임의로 정한 숫자다. 정답이 없고 실험적으로 파악해야 한다. 보통 0과 1사이의 작은 숫자가 들어간다.

효율적으로 최저점을 찾기위함 learning rate optimizer가 있다.
    - Momentum : 관성. 가속도를 유지하자.
    - AdaGrad : 자주변하는 w는 작게, 자주변하면 크게
    - RMSProp : AdaGrad의 제곱
    - AdaDelta : AdaGrad에서 a가 너무작아서 학습안될때를 방지
    - **Adam** : RMSProp + Momentum (대부분 이것을 사용)


#### 텐서플로우
> 구글에서 개발
> 기본단위가 텐서다. multi-demensional array를 나타내는 말. (다차원배열)
import tensorflow as tf
텐서 = tf.constant(3)   //텐서플로우에서 숫자와 리스트 담는 자료형

- 텐서가 필요한 이유
  : 행렬로 인풋/ w값 저장가능 
        -> node값 계산식이 매우 쉬워짐

- 텐서 함수
  - add(), subtract(), multiply(), divide()
  - matmul()       : 행렬곱
  - zeros(모양)    : 0이 모양만큼 가득담은 행렬 반환
    예) zeros([2,2])    : 2 x 2행렬을 0으로 가득채워 반환
  - cast()          : 형변환
  - Variable(초기값)      : V 대문자에 유의.  변수를 만듬. 주로 weight를 저장할때 사용
  -       - numpy()     : 변수값 호출
      - assign()    : 변수에 값 할당
- 속성
  - shape


### 실습. 키로 신발크기 경사하강법으로 구하기
```python
import tensorflow as tf
키 = 170
신발 = 260

a = tf.Variable(0.1)
b = tf.Variable(0.2)

def 손실함수():
    예측값 = 
    return tf.square(실제값 - 예측값 )



opt = tf.keras.optimizers.Adam(learning_rate = 0.1)
#opt.minimize(손실함수, 업데이트할 가중치변수 목록)
for i in range(300):
    opt.minimize(손실함수, 업데이트할 가중치변수 목록)
    print(a.numpy(),b.numpy())
    # a는 1.52   b는 1.62 나온다.
```


### 케라스  ### keras
텐서플로우같은걸 좀더 쓰기쉽게 하기 위한 고레벨 라이브러리
아예 텐서플로우 안에 케라스가 포함이 되어버렸다.




